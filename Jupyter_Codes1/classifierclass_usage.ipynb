{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Course Instructor`: **John Chiasson**\n",
    "\n",
    "`Author (TA)`: **Ruthvik Vaila**\n",
    "\n",
    "# References\n",
    "* This notebook shows how to use the `nn_classifierclass.py` which is a wrapper for `Keras` which itself is a wrapper for `Tensorflow`. We can use `nn_classifierclass.py` to quickly train fully connected neural networks. Some of the below links might not be really necessary for this notebook nevertheless these are good reads.  \n",
    "* [Neural Nets](http://neuralnetworksanddeeplearning.com/chap3.html)\n",
    "* [Randombackprop](https://github.com/xuexue/randombp/blob/master/randombp.py)\n",
    "* [Randombackprop](https://github.com/sangyi92/feedback_alignment/blob/master/RFA.ipynb)\n",
    "* [Backprop](http://blog.aloni.org/posts/backprop-with-tensorflow/)\n",
    "* [Initializers](https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404)\n",
    "* [Dropout](https://github.com/pinae/TensorFlow-MNIST-example/blob/master/fully-connected.py)\n",
    "* [Softmax](https://stackoverflow.com/questions/34240703/what-is-logits-softmax-and-softmax-cross-entropy-with-logits)\n",
    "* [SoftmaxLogits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "* Tested on `Python 3.7.5` with `Tensorflow 1.15.0` and `Keras 2.2.4`. \n",
    "* Tested on `Python 2.7.17` with `Tensorflow 1.15.3` and `Keras 2.2.4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py, sys, os, time, pickle, gzip\n",
    "GPU =  False\n",
    "if(not GPU):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend\n",
    "import IPython\n",
    "from tensorflow.python.client import device_lib\n",
    "import nn_classifierclass as cls\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 5712067084846844192]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# IGNORE THIS CELL FOR NOW\n",
    "filename = 'data/emnist_train_x.h5'\n",
    "with h5py.File(filename, 'r') as hf:\n",
    "    train_x = hf['pool1_spike_features'][:]\n",
    "\n",
    "filename = 'data/emnist_test_x.h5'\n",
    "with h5py.File(filename, 'r') as hf:\n",
    "    test_x = hf['pool1_spike_features'][:]\n",
    "\n",
    "print('Train data shape:{}'.format(train_x.shape))\n",
    "print('Test data shape:{}'.format(test_x.shape))\n",
    "\n",
    "filename = 'data/emnist_train_y.pkl'\n",
    "filehandle = open(filename, 'rb')\n",
    "train_y = pickle.load(filehandle)\n",
    "filehandle.close()\n",
    "\n",
    "filename = 'data/emnist_test_y.pkl'\n",
    "filehandle = open(filename, 'rb')\n",
    "test_y = pickle.load(filehandle)\n",
    "filehandle.close()\n",
    "\n",
    "print('Train labels shape:{}'.format(train_y.shape))\n",
    "print('Test labels shape:{}'.format(test_y.shape))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:(50000, 784) and labels shape:(50000,)\n",
      "Valid data shape:(10000, 784) and labels shape:(10000,)\n",
      "Train data shape:(60000, 784)\n",
      "Train labels shape:(60000,)\n",
      "Test data shape:(10000, 784)\n",
      "Test labels shape:(10000,)\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/mnist.pkl.gz'\n",
    "filehandle = gzip.open(filename, 'rb')\n",
    "train_data, val_data, test_data = pickle.load(filehandle,encoding = 'latin1')\n",
    "filehandle.close()\n",
    "train_x, train_y = train_data\n",
    "print('Train data shape:{} and labels shape:{}'.format(train_x.shape, train_y.shape))\n",
    "val_x, val_y = val_data\n",
    "print('Valid data shape:{} and labels shape:{}'.format(val_x.shape, val_y.shape))\n",
    "## combine train and validation data, classifier_class can split it inside \n",
    "train_x = np.concatenate([train_x, val_x], axis=0)\n",
    "train_y = np.concatenate([train_y, val_y], axis=0)\n",
    "print('Train data shape:{}'.format(train_x.shape))\n",
    "print('Train labels shape:{}'.format(train_y.shape))\n",
    "test_x, test_y = test_data\n",
    "print('Test data shape:{}'.format(test_x.shape))\n",
    "print('Test labels shape:{}'.format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a NN classifier and train with `Keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./logs/relusoftmax-glorot_uniform-plateau-0.0/eta-0.005/\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "n_hidden = 1 #number of hidden layers\n",
    "network_structure = [train_x.shape[1],1500,n_classes]\n",
    "#activation_fns = ['sigmoid']*(n_hidden)+['softmax']\n",
    "#activation_fns = ['tanh']*(n_hidden)+['softmax']\n",
    "activation_fns = ['relu']*(n_hidden)+['softmax']\n",
    "#activation_fns = ['swish']*(n_hidden)+['softmax']\n",
    "#activation_fns = ['softmax']\n",
    "#sys.exit()\n",
    "#weight_init = 'he_uniform'\n",
    "weight_init = 'glorot_uniform'\n",
    "eta_drop_type = 'plateau'\n",
    "lmbda = 0.000\n",
    "batch_size = 32  # mini-batch size\n",
    "eta = 0.005\n",
    "\n",
    "log_path = r'./logs'+''.join(['/']+activation_fns+['-',weight_init,'-',eta_drop_type,str(-lmbda)])+'/eta'+str(-eta)+'/'\n",
    "print(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat:0\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1500)              1177500   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                15010     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,192,510\n",
      "Trainable params: 1,192,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 54600 samples, validate on 5400 samples\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\callbacks.py:796: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/3\n",
      "54600/54600 [==============================] - 31s 574us/step - loss: 0.2265 - acc: 0.9348 - val_loss: 0.1114 - val_acc: 0.9648\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96481, saving model to weights.best.hdf5\n",
      "WARNING:tensorflow:From C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\callbacks.py:995: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/3\n",
      "54600/54600 [==============================] - 31s 575us/step - loss: 0.1122 - acc: 0.9689 - val_loss: 0.1340 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96481 to 0.96944, saving model to weights.best.hdf5\n",
      "Epoch 3/3\n",
      "54600/54600 [==============================] - 33s 598us/step - loss: 0.0847 - acc: 0.9759 - val_loss: 0.0841 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.96944 to 0.97630, saving model to weights.best.hdf5\n",
      "Testing model at the end\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "acc: 97.47%\n",
      "Testing the best model\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "acc: 97.47%\n"
     ]
    }
   ],
   "source": [
    "repeats = 1\n",
    "all_histories = []\n",
    "for repeat in range(repeats):\n",
    "    print('Repeat:{}'.format(repeat))\n",
    "    backend.clear_session()\n",
    "    neural_net = cls.Classifier(train_data=(train_x,train_y),\n",
    "                                test_data=(test_x,test_y),\n",
    "                                network_structure=network_structure,activation_fns=activation_fns,\n",
    "                                epochs=3,eta=eta,lmbda=lmbda,verbose=1,plots=False,optimizer='adam',\n",
    "                                eta_decay_factor=1.007,patience=8,eta_drop_type=eta_drop_type,\n",
    "                                epochs_drop=1, val_frac=0.09,drop_out=0.0,ip_lyr_drop_out=0.0,\n",
    "                                leaky_alpha=0.1,leaky_relu=False,weight_init=weight_init,\n",
    "                                bias_init=0.1,batch_size=batch_size,log_path=log_path)\n",
    "    neural_net.keras_fcn_classifier()\n",
    "    all_histories.append(neural_net.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Visualization\n",
    "* This cell works **only** in Linux. Do not run in Windows!\n",
    "* For Windows instructions see the **Issues.pdf** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir {log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logs/relusoftmax-glorot_uniform-plateau-0.0/eta-0.005/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best test:97.47, mean test:54.12074801295177, std test:43.34925198704823\n",
      "best final test:97.47, mean final test:54.12074801295177, std final test:43.34925198704823\n"
     ]
    }
   ],
   "source": [
    "best_accuracies = [item['best_test_acc'] for item in all_histories]\n",
    "last_accuracies = [item['last_test_acc'] for item in all_histories]\n",
    "best_accuracies = np.array(best_accuracies)*100\n",
    "print('best test:{}, mean test:{}, std test:{}'.format(best_accuracies.max(),\n",
    "                                                                        best_accuracies.mean(),\n",
    "                                                                         best_accuracies.std()))\n",
    "\n",
    "last_accuracies = np.array(last_accuracies)*100\n",
    "print('best final test:{}, mean final test:{}, std final test:{}'.format(last_accuracies.max(),\n",
    "                                                                        last_accuracies.mean(),\n",
    "                                                                         last_accuracies.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle file written to:./logs/relusoftmax-glorot_uniform-plateau-0.0/eta-0.005/results.pkl\n"
     ]
    }
   ],
   "source": [
    "picklefile = log_path + 'results.pkl' \n",
    "output1 = open(picklefile,'wb')\n",
    "pickle.dump(all_histories,output1)\n",
    "output1.close()\n",
    "print('pickle file written to:{}'.format(picklefile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the written data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "picklefile = open(log_path + 'results.pkl', 'rb') \n",
    "loaded_spike_count_record_all_histories = pickle.load(picklefile)\n",
    "picklefile.close()\n",
    "#loaded_spike_count_record_all_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a NN classifier and train with `NumPy`\n",
    "* Note that this method trains only with `softmax` neurons and it cannot log data like the above `keras` version.\n",
    "* To add more activations functions add corresponding functions to the file `numpy_fcn.py` file, add `if else` conditions to choose the activation function based on user input.\n",
    "* `nn_classifierclass.numpy_fcn_classifier` is a wrapper around Michael Nielsen's `NumPy` based bare bones implementation.\n",
    "* Runs only on the `CPU`.\n",
    "* The code in `numpy_fcn.py` was written by [Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html) and it is slow because it was written with understandability of backpropagation implementation in mind and NOT speed or vectorization of mini-batch computations on the CPU. Specifically see the following lines in `numpy_fcn.py`. **WARNING:** This part runs very slow with `EMNIST` dataset use only `MNIST` for this.\n",
    "\n",
    "`\n",
    "200             for mini_batch in mini_batches:\n",
    "201                 self.update_mini_batch(\n",
    "202                     mini_batch, eta, lmbda, len(training_data))\n",
    "`\n",
    "* See the below lines where we loop over each sample in the mini-batch, instead this can be vectorized. Use the same logic as in the homework problem related to a simple neural network separating a spirally distributed data points.\n",
    "\n",
    "`\n",
    "246         for x, y in mini_batch:\n",
    "247             delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "248             nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "249             nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "250         self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "251                         for w, nw in zip(self.weights, nabla_w)]\n",
    "252         self.biases = [b-(eta/len(mini_batch))*nb\n",
    "253                        for b, nb in zip(self.biases, nabla_b)]\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/visionteam/tf_tutorials/logs/swishsoftmax-glorot_uniform-plateau-0.0/eta-0.005\n",
      "Repeat:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:174: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:191: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:174: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\kalin\\Miniconda3\\envs\\deeplearn\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:191: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "[                                                                        ] N/A%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta for epoch: 0 is: 0.005\n",
      "Epoch 0 progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: -0.000325125536201707\n",
      "Accuracy on training data: 0.8438095238095238 / 54600, 0.8438095238095238\n",
      "Cost on evaluation data: -0.0003229550875535708\n",
      "Accuracy on evaluation data: 0.8494444444444444 \n",
      "saved network to:fcn_best_validation_network_rand_epochs_1_lambda_0.0\n",
      "test accuracy at the end:0.8545\n",
      "best validation accuracy is:0.8494444444444444\n",
      "test accuracy for weights corresponding to the best validation accuracy:0.8545\n"
     ]
    }
   ],
   "source": [
    "n_classes = 47\n",
    "n_hidden = 1\n",
    "network_structure = [train_x.shape[1],1500,n_classes] ##will be ignored, only sigmoid neurons\n",
    "#activation_fns = ['sigmoid']*(n_hidden)+['softmax']\n",
    "#activation_fns = ['tanh']*(n_hidden)+['softmax']\n",
    "#activation_fns = ['relu']*(n_hidden)+['softmax']\n",
    "activation_fns = ['swish']*(n_hidden)+['softmax'] #will be ignored, only sigmoid neurons and last layer is softmax\n",
    "#activation_fns = ['softmax']\n",
    "#sys.exit()\n",
    "#weight_init = 'he_uniform'\n",
    "weight_init = 'glorot_uniform'\n",
    "eta_drop_type = 'plateau'\n",
    "lmbda = 0.000\n",
    "batch_size = 32\n",
    "eta = 0.005\n",
    "\n",
    "log_path = '/home/visionteam/tf_tutorials/logs/'+''.\\\n",
    "join(activation_fns+['-',weight_init,'-',eta_drop_type,str(-lmbda)])+'/eta'+str(-eta)\n",
    "print(log_path)\n",
    "repeats = 1\n",
    "all_histories = []\n",
    "for repeat in range(repeats):\n",
    "    print('Repeat:{}'.format(repeat))\n",
    "    backend.clear_session()\n",
    "    neural_net = cls.Classifier(train_data=(train_x,train_y),\n",
    "                                test_data=(test_x,test_y),\n",
    "                                network_structure=network_structure,activation_fns=activation_fns,\n",
    "                                epochs=1,eta=eta,lmbda=lmbda,verbose=1,plots=False,optimizer='adam',\n",
    "                                eta_decay_factor=1.007,patience=8,eta_drop_type=eta_drop_type,\n",
    "                                epochs_drop=1, val_frac=0.09,drop_out=0.0,ip_lyr_drop_out=0.0,\n",
    "                                leaky_alpha=0.1,leaky_relu=False,weight_init=weight_init,\n",
    "                                bias_init=0.1,batch_size=batch_size,log_path=log_path)\n",
    "    neural_net.numpy_fcn_classifier()\n",
    "    all_histories.append(neural_net.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart the notebook to free up the `GPU` and `RAM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "* Write a method `keras_cnn_classifer` in `classifier_class.py` that can train a Convolutional Neural Network.\n",
    "* Write a class `Vecotized_Network` in `numpy_fcn.py` so that we can have a vectorized implementation, see cell number 6 for tips on where to look for modifications to be done.\n",
    "* Plot various metrics like `training cost vs epochs`, `training accuracy vs epochs` etc. These metrics can be found in the attribute `self.history` of the `nn_classiferclass.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
